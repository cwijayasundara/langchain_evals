{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain -q\n",
    "%pip install langchain_chroma -q\n",
    "%pip install langchain_community -q\n",
    "%pip install langchain_groq -q\n",
    "%pip install grandalf -q\n",
    "%pip install numpy -q\n",
    "%pip install pandas -q\n",
    "%pip install sentence-transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import warnings\n",
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "_ = load_dotenv()\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "embed_model = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "rag_llm = ChatGroq(model=\"llama-3.2-3b-preview\") # Used for RAG\n",
    "qa_llm = ChatGroq(model=\"llama-3.2-11b-text-preview\", temperature=0.1) # Used to create eval dataset\n",
    "benchmark_llm = ChatGroq(model=\"llama-3.2-90b-text-preview\") # Used to evaluate (Judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents indexed: 27\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Use popular github repo instead\n",
    "loader = DirectoryLoader(\"data/paul_graham/\", use_multithreading=True, loader_cls=TextLoader)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\n",
    "        \"\\n\\n\", \n",
    "        \"\\n\", \n",
    "        \" \",\n",
    "        \"\",\n",
    "    ],\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "documents = loader.load_and_split(text_splitter=text_splitter) # Load text\n",
    "vectorstore = Chroma.from_documents(documents, embedding=embed_model, collection_name=\"groq_rag\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(f\"Documents indexed: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/paul_graham/paul_graham_essay.txt'}, page_content=\"Asterix comics begin by zooming in on a tiny corner of Roman Gaul that turns out not to be controlled by the Romans. You can do something similar on a map of New York City: if you zoom in on the Upper East Side, there's a tiny corner that's not rich, or at least wasn't in 1993. It's called Yorkville, and that was my new home. Now I was a New York artist â€” in the strictly technical sense of making paintings and living in New York.\\n\\nI was nervous about money, because I could sense that Interleaf was on the way down. Freelance Lisp hacking work was very rare, and I didn't want to have to program in another language, which in those days would have meant C++ if I was lucky. So with my unerring nose for financial opportunity, I decided to write another book on Lisp. This would be a popular book, the sort of book that could be used as a textbook. I imagined myself living frugally off the royalties and spending all my time painting. (The painting on the cover of this book, ANSI Common Lisp, is one that I painted around this time.)\\n\\nThe best thing about New York for me was the presence of Idelle and Julian Weber. Idelle Weber was a painter, one of the early photorealists, and I'd taken her painting class at Harvard. I've never known a teacher more beloved by her students. Large numbers of former students kept in touch with her, including me. After I moved to New York I became her de facto studio assistant.\\n\\nShe liked to paint on big, square canvases, 4 to 5 feet on a side. One day in late 1994 as I was stretching one of these monsters there was something on the radio about a famous fund manager. He wasn't that much older than me, and was super rich. The thought suddenly occurred to me: why don't I become rich? Then I'll be able to work on whatever I want.\\n\\nMeanwhile I'd been hearing more and more about this new thing called the World Wide Web. Robert Morris showed it to me when I visited him in Cambridge, where he was now in grad school at Harvard. It seemed to me that the web would be a big deal. I'd seen what graphical user interfaces had done for the popularity of microcomputers. It seemed like the web would do the same for the internet.\\n\\nIf I wanted to get rich, here was the next train leaving the station. I was right about that part. What I got wrong was the idea. I decided we should start a company to put art galleries online. I can't honestly say, after reading so many Y Combinator applications, that this was the worst startup idea ever, but it was up there. Art galleries didn't want to be online, and still don't, not the fancy ones. That's not how they sell. I wrote some software to generate web sites for galleries, and Robert wrote some to resize images and set up an http server to serve the pages. Then we tried to sign up galleries. To call this a difficult sale would be an understatement. It was difficult to give away. A few galleries let us make sites for them for free, but none paid us.\"),\n",
       " Document(metadata={'source': 'data/paul_graham/paul_graham_essay.txt'}, page_content='I had to ban myself from writing essays during most of this time, or I\\'d never have finished. In late 2015 I spent 3 months writing essays, and when I went back to working on Bel I could barely understand the code. Not so much because it was badly written as because the problem is so convoluted. When you\\'re working on an interpreter written in itself, it\\'s hard to keep track of what\\'s happening at what level, and errors can be practically encrypted by the time you get them.\\n\\nSo I said no more essays till Bel was done. But I told few people about Bel while I was working on it. So for years it must have seemed that I was doing nothing, when in fact I was working harder than I\\'d ever worked on anything. Occasionally after wrestling for hours with some gruesome bug I\\'d check Twitter or HN and see someone asking \"Does Paul Graham still code?\"\\n\\nWorking on Bel was hard but satisfying. I worked on it so intensively that at any given time I had a decent chunk of the code in my head and could write more there. I remember taking the boys to the coast on a sunny day in 2015 and figuring out how to deal with some problem involving continuations while I watched them play in the tide pools. It felt like I was doing life right. I remember that because I was slightly dismayed at how novel it felt. The good news is that I had more moments like this over the next few years.\\n\\nIn the summer of 2016 we moved to England. We wanted our kids to see what it was like living in another country, and since I was a British citizen by birth, that seemed the obvious choice. We only meant to stay for a year, but we liked it so much that we still live there. So most of Bel was written in England.\\n\\nIn the fall of 2019, Bel was finally finished. Like McCarthy\\'s original Lisp, it\\'s a spec rather than an implementation, although like McCarthy\\'s Lisp it\\'s a spec expressed as code.\\n\\nNow that I could write essays again, I wrote a bunch about topics I\\'d had stacked up. I kept writing essays through 2020, but I also started to think about other things I could work on. How should I choose what to do? Well, how had I chosen what to work on in the past? I wrote an essay for myself to answer that question, and I was surprised how long and messy the answer turned out to be. If this surprised me, who\\'d lived it, then I thought perhaps it would be interesting to other people, and encouraging to those with similarly messy lives. So I wrote a more detailed version for others to read, and this is the last sentence of it.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNotes\\n\\n[1] My experience skipped a step in the evolution of computers: time-sharing machines with interactive OSes. I went straight from batch processing to microcomputers, which made microcomputers seem all the more exciting.'),\n",
       " Document(metadata={'source': 'data/paul_graham/paul_graham_essay.txt'}, page_content='What I Worked On\\n\\nFebruary 2021\\n\\nBefore college the two main things I worked on, outside of school, were writing and programming. I didn\\'t write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\\n\\nThe first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district\\'s 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain\\'s lair down there, with all these alien-looking machines â€” CPU, disk drives, printer, card reader â€” sitting up on a raised floor under bright fluorescent lights.\\n\\nThe language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\\n\\nI was puzzled by the 1401. I couldn\\'t figure out what to do with it. And in retrospect there\\'s not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn\\'t have any data stored on punched cards. The only other option was to do things that didn\\'t rely on any input, like calculate approximations of pi, but I didn\\'t know enough math to do anything interesting of that type. So I\\'m not surprised I can\\'t remember any programs I wrote, because they can\\'t have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn\\'t. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager\\'s expression made clear.\\n\\nWith microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\\n\\nThe first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\\n\\nComputers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he\\'d write 2 pages at a time and then print them out, but it was a lot better than a typewriter.'),\n",
       " Document(metadata={'source': 'data/paul_graham/paul_graham_essay.txt'}, page_content='At this stage I had a negative net worth, because the thousand dollars or so I had in the bank was more than counterbalanced by what I owed the government in taxes. (Had I diligently set aside the proper proportion of the money I\\'d made consulting for Interleaf? No, I had not.) So although Robert had his graduate student stipend, I needed that seed funding to live on.\\n\\nWe originally hoped to launch in September, but we got more ambitious about the software as we worked on it. Eventually we managed to build a WYSIWYG site builder, in the sense that as you were creating pages, they looked exactly like the static ones that would be generated later, except that instead of leading to static pages, the links all referred to closures stored in a hash table on the server.\\n\\nIt helped to have studied art, because the main goal of an online store builder is to make users look legit, and the key to looking legit is high production values. If you get page layouts and fonts and colors right, you can make a guy running a store out of his bedroom look more legit than a big company.\\n\\n(If you\\'re curious why my site looks so old-fashioned, it\\'s because it\\'s still made with this software. It may look clunky today, but in 1996 it was the last word in slick.)\\n\\nIn September, Robert rebelled. \"We\\'ve been working on this for a month,\" he said, \"and it\\'s still not done.\" This is funny in retrospect, because he would still be working on it almost 3 years later. But I decided it might be prudent to recruit more programmers, and I asked Robert who else in grad school with him was really good. He recommended Trevor Blackwell, which surprised me at first, because at that point I knew Trevor mainly for his plan to reduce everything in his life to a stack of notecards, which he carried around with him. But Rtm was right, as usual. Trevor turned out to be a frighteningly effective hacker.\\n\\nIt was a lot of fun working with Robert and Trevor. They\\'re the two most independent-minded people I know, and in completely different ways. If you could see inside Rtm\\'s brain it would look like a colonial New England church, and if you could see inside Trevor\\'s it would look like the worst excesses of Austrian Rococo.\\n\\nWe opened for business, with 6 stores, in January 1996. It was just as well we waited a few months, because although we worried we were late, we were actually almost fatally early. There was a lot of talk in the press then about ecommerce, but not many people actually wanted online stores. [8]')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await retriever.ainvoke(\"What did paul graham do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from typing import List, Dict\n",
    "\n",
    "RAG_SYSTEM_PROMPT = \"\"\"\\\n",
    "You are an assistant for question-answering tasks. \\\n",
    "Use the following pieces of retrieved context given within delimiters to answer the human's questions.\n",
    "```\n",
    "{context}\n",
    "```\n",
    "If you don't know the answer, just say that you don't know.\\\n",
    "\"\"\" # adapted from https://smith.langchain.com/hub/rlm/rag-prompt-llama3\n",
    "\n",
    "RAG_HUMAN_PROMPT = \"{input}\"\n",
    "\n",
    "RAG_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", RAG_SYSTEM_PROMPT),\n",
    "    (\"human\", RAG_HUMAN_PROMPT)\n",
    "])\n",
    "\n",
    "def format_docs(docs: List[Document]):\n",
    "    \"\"\"Format the retrieved documents\"\"\"\n",
    "    return \"\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"context\": retriever | format_docs, # Use retriever to retrieve docs from vectorstore -> format the documents into a string\n",
    "        \"input\": RunnablePassthrough() # Propogate the 'input' variable to the next step\n",
    "    } \n",
    "    | RAG_PROMPT # format prompt with 'context' and 'input' variables\n",
    "    | rag_llm # get response from LLM using the formatteed prompt\n",
    "    | StrOutputParser() # Parse through LLM response to get only the string response\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the given context, before college, Paul Graham worked on writing and programming. \\n\\nSpecifically, he started programming in 9th grade (around 13 or 14 years old) on the IBM 1401, a data processing computer, using an early version of Fortran. He also wrote short stories and later, he started writing games and other simple programs on a TRS-80 microcomputer, which he got in about 1980.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await rag_chain.ainvoke(\"What did paul graham do growing up?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Evaluation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "# Response object structure\n",
    "class QAResponse(TypedDict):\n",
    "    question_1: str\n",
    "    question_2: str\n",
    "    question_3: str\n",
    "\n",
    "QA_HUMAN_PROMPT = \"\"\"\\\n",
    "You are a Teacher/ Professor. Your task is to setup questions for an upcoming \\\n",
    "quiz/examination. The questions should be diverse in nature across the document. \\\n",
    "Given the context information and not prior knowledge, generate only questions based on the below context. \\\n",
    "Restrict the questions to the context information provided within the delimiters.\n",
    "```\n",
    "{text}\n",
    "```\n",
    "Output the questions in JSON format with the keys question_1, question_2 and question_3 \\\n",
    "and make sure to escape any special characters to output clean, valid JSON.\\\n",
    "\"\"\" # adapted from https://arize.com/blog/evaluate-rag-with-llm-evals-and-benchmarking/\n",
    "\n",
    "QA_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", QA_HUMAN_PROMPT)\n",
    "])\n",
    "\n",
    "qa_chain = (\n",
    "{\"text\": RunnablePassthrough()}\n",
    "| QA_PROMPT\n",
    "| qa_llm.with_structured_output(method='json_mode', schema=QAResponse) # either 'json_mode' or 'function_calling' to get responses always in JSON format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [doc.page_content for doc in documents] # Create a list with all the text from the document chunks in the vectorstore\n",
    "questions: List[Dict] = await qa_chain.abatch(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From document: \n",
      "What I Worked On\n",
      "\n",
      "February 2021\n",
      "\n",
      "Before college the two main things I worked on, outside of school, were writing and programming. I didn't write essays. I wrote what beginning writers were supposed to write then, and probably still are: short stories. My stories were awful. They had hardly any plot, just characters with strong feelings, which I imagined made them deep.\n",
      "\n",
      "The first programs I tried writing were on the IBM 1401 that our school district used for what was then called \"data processing.\" This was in 9th grade, so I was 13 or 14. The school district's 1401 happened to be in the basement of our junior high school, and my friend Rich Draves and I got permission to use it. It was like a mini Bond villain's lair down there, with all these alien-looking machines â€” CPU, disk drives, printer, card reader â€” sitting up on a raised floor under bright fluorescent lights.\n",
      "\n",
      "The language we used was an early version of Fortran. You had to type programs on punch cards, then stack them in the card reader and press a button to load the program into memory and run it. The result would ordinarily be to print something on the spectacularly loud printer.\n",
      "\n",
      "I was puzzled by the 1401. I couldn't figure out what to do with it. And in retrospect there's not much I could have done with it. The only form of input to programs was data stored on punched cards, and I didn't have any data stored on punched cards. The only other option was to do things that didn't rely on any input, like calculate approximations of pi, but I didn't know enough math to do anything interesting of that type. So I'm not surprised I can't remember any programs I wrote, because they can't have done much. My clearest memory is of the moment I learned it was possible for programs not to terminate, when one of mine didn't. On a machine without time-sharing, this was a social as well as a technical error, as the data center manager's expression made clear.\n",
      "\n",
      "With microcomputers, everything changed. Now you could have a computer sitting right in front of you, on a desk, that could respond to your keystrokes as it was running instead of just churning through a stack of punch cards and then stopping. [1]\n",
      "\n",
      "The first of my friends to get a microcomputer built it himself. It was sold as a kit by Heathkit. I remember vividly how impressed and envious I felt watching him sitting in front of it, typing programs right into the computer.\n",
      "\n",
      "Computers were expensive in those days and it took me years of nagging before I convinced my father to buy one, a TRS-80, in about 1980. The gold standard then was the Apple II, but a TRS-80 was good enough. This was when I really started programming. I wrote simple games, a program to predict how high my model rockets would fly, and a word processor that my father used to write at least one book. There was only room in memory for about 2 pages of text, so he'd write 2 pages at a time and then print them out, but it was a lot better than a typewriter.\n",
      "\n",
      "Questions generated:\n",
      "1: What were the two main things the author worked on outside of school before college?\n",
      "2: What programming language was used on the IBM 1401, and what was the process of running a program on this machine?\n",
      "3: What was the first microcomputer that the author's friend built, and what was the author's reaction to seeing it in action?\n"
     ]
    }
   ],
   "source": [
    "print(f\"From document: \\n{texts[0]}\\n\")\n",
    "print(f\"Questions generated:\")\n",
    "for i, q in enumerate(questions[0].values(), 1): print(f'{i}: {q}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the RAG Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "# Response object structure\n",
    "class EvalResponse(BaseModel):\n",
    "    score: int\n",
    "    explanation: str\n",
    "\n",
    "EVAL_HUMAN_PROMPT = \"\"\"\\\n",
    "You are given a question, an answer and reference text within marked delimiters. \\\n",
    "You must determine whether the given answer correctly answers the question based on the reference text. Here is the data:\n",
    "```Question\n",
    "{question}\n",
    "```\n",
    "```Reference\n",
    "{context}\n",
    "```\n",
    "```Answer\n",
    "{answer}\n",
    "```\n",
    "Respond with a valid JSON object containing two fields:\n",
    "{{\n",
    "    \"score\": \"int: a score between 0-10, 10 being highest, on whether the question is correctly and fully answered by the answer\",\n",
    "    \"explanation\": \"str: Provide an explanation as to why the score was given.\"\n",
    "}} \n",
    "Make sure to escape any special characters to output clean, valid JSON.\\\n",
    "\"\"\"\n",
    "# adapted from https://docs.arize.com/phoenix/evaluation/how-to-evals/running-pre-tested-evals/q-and-a-on-retrieved-data\n",
    "\n",
    "EVAL_PROMPT = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", EVAL_HUMAN_PROMPT)\n",
    "])\n",
    "\n",
    "eval_chain = (\n",
    "    {\n",
    "    \"context\": RunnablePassthrough(), # Propogate all input vars to next step in pipeline\n",
    "    \"question\": RunnablePassthrough(), \n",
    "    \"answer\": RunnablePassthrough(),\n",
    "    }\n",
    "    | EVAL_PROMPT\n",
    "    | benchmark_llm.with_structured_output(schema=EvalResponse, method='json_mode') # Parse response according to EvalResponse object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What was a major issue with HN (Hacker News) when you both wrote essays and ran a forum?\n",
      "Answer: A major issue with Hacker News (HN) when you both wrote essays and ran a forum was that you had to respond to highly upvoted misinterpretations of your essays, which created a vicious cycle that encouraged more misinterpretation and conflict.\n",
      "---------------------\n",
      "Score: 9\n",
      "Explanation: The answer accurately summarizes the major issue with HN when both writing essays and running a forum, as described in the reference text. It correctly identifies the problem of having to respond to highly upvoted misinterpretations of essays, which creates a vicious cycle. The only reason the score isn't a perfect 10 is that the answer could be more concise and directly quote the reference text for added clarity.\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "q1 = questions[-1]['question_1']\n",
    "t1 = texts[-1]\n",
    "\n",
    "print(f\"Question: {q1}\")\n",
    "a1 = await rag_chain.ainvoke(q1)\n",
    "print(f\"Answer: {a1}\")\n",
    "eval_input = {\n",
    "    'context': t1,\n",
    "    'question': q1,\n",
    "    'answer': a1\n",
    "}\n",
    "response = await eval_chain.ainvoke(eval_input)\n",
    "print(\"---------------------\")\n",
    "print(f\"Score: {response.score}\")\n",
    "print(f\"Explanation: {response.explanation}\")\n",
    "print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets do it for all now\n",
    "from typing import TypedDict\n",
    "from time import time\n",
    "\n",
    "class EvalResult(TypedDict): # For type hinting\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: str\n",
    "    score: int # Score between 0 - 10\n",
    "    explanation: str # Explanation on why the score was given\n",
    "\n",
    "async def evaluate(questions: List[Dict] = questions, texts: List[str] = texts) -> List[EvalResult]:\n",
    "    # Prepare inputs\n",
    "    batch_rag_inputs: List[Dict] = []\n",
    "    evals: List[Dict] = []\n",
    "    for q_dict, context in zip(questions, texts): \n",
    "        for question in q_dict.values(): \n",
    "            batch_rag_inputs.append(question)\n",
    "            evals.append({'question': question, 'context': context})\n",
    "\n",
    "    print(f\"Running RAG pipeline for {len(batch_rag_inputs)} questions\")\n",
    "    start = time()\n",
    "    answers = await rag_chain.abatch(batch_rag_inputs, config={'max_concurrency': 2}) # Reduce concurrency to avoid hitting rate limits\n",
    "    end = time()\n",
    "    print(f\"Time taken: {end - start}\")\n",
    "\n",
    "    # Update eval_input with the answers from the rag_chain\n",
    "    for eval_input, answer in zip(evals, answers):\n",
    "        eval_input.update({'answer': answer})\n",
    "    \n",
    "    # Run eval_chain to get evaluation\n",
    "    print(f\"Evaluating RAG pipeline...\")\n",
    "    start = time()\n",
    "    batch_score_explanations = await eval_chain.abatch(evals, config={'max_concurrency': 2}) # Pass in eval which contains List of 'answer', 'context', 'question'\n",
    "    end = time()\n",
    "    print(f\"Time taken: {end - start}\")\n",
    "    \n",
    "    # Update eval variable with the score and explanation\n",
    "    for eval, score_exp_dict in zip(evals, batch_score_explanations):\n",
    "        eval.update({\n",
    "            'score': score_exp_dict['score'],\n",
    "            'explanation': score_exp_dict['explanation']\n",
    "        })\n",
    "    \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = await evaluate(questions[:1], texts[:1]) # Remove the `:5` to evaluate all the questions on all your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display and Save the Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m score_threshold  \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Display all results below 5\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Calculating basic statistics\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m scores \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28meval\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28meval\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mevaluations\u001b[49m]\n\u001b[1;32m      8\u001b[0m average_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(scores) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(scores)\n\u001b[1;32m      9\u001b[0m std_dev_score \u001b[38;5;241m=\u001b[39m statistics\u001b[38;5;241m.\u001b[39mstdev(scores)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluations' is not defined"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import statistics\n",
    "\n",
    "score_threshold  = 1 # Display all results below 5\n",
    "\n",
    "# Calculating basic statistics\n",
    "scores = [eval['score'] for eval in evaluations]\n",
    "average_score = sum(scores) / len(scores)\n",
    "std_dev_score = statistics.stdev(scores)\n",
    "\n",
    "# Lowest and highest scores\n",
    "lowest_score = min(scores)\n",
    "highest_score = max(scores)\n",
    "lowest_count = scores.count(lowest_score)\n",
    "highest_count = scores.count(highest_score)\n",
    "\n",
    "# Display results\n",
    "print(\"Average Score:\", average_score)\n",
    "print(\"Standard Deviation of Score:\", std_dev_score)\n",
    "print(\"Lowest Score:\", lowest_score, \"Count:\", lowest_count)\n",
    "print(\"Highest Score:\", highest_score, \"Count:\", highest_count)\n",
    "\n",
    "print(f\"Evals lower than {score_threshold}\")\n",
    "count = 1\n",
    "for eval in evaluations:\n",
    "    if eval['score'] <= score_threshold:\n",
    "        print(\"--------------------------\")\n",
    "        print(f\"{count}. Score: {eval['score']}\")\n",
    "        print(f\"Question: {eval['question']}\")\n",
    "        print(f\"Answer: {eval['answer']}\")\n",
    "        print(f\"Explanation: {eval['explanation']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = 'evaluations.csv'\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Context', 'Score', 'Question', 'Answer', 'Explanation'])\n",
    "    for eval in evaluations:\n",
    "        writer.writerow([eval['context'], eval['score'], eval['question'], eval['answer'], eval['explanation']])\n",
    "\n",
    "print(f\"Evaluations saved to {csv_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
